# -*- coding: utf-8 -*-
"""EXAM Review.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_MYbbU_ue6jGuEOYBkOZ4KXMAa3hH75O

- Q1. Class
"""

class Parent:
  def __init__(self, p1, p2):
    self.p1 = p1
    self.p2 = p2

  p3 = "Dummy" # Class 전역 변수는 self 사용하지 않음 

class Child_1(Parent):
  def __init__(self, b1, **kwargs):
    self.b1 = b1

class Child_2(Parent):
  def __init__(self, c1, **kwargs)   :
    super().__init__(**kwargs)
    self.c1 = c1

A = Parent(10, 20)
B = Child_1(100, p1 = 30, p2 = 40)
C = Child_2(100, p1 = 50, p2 = 60)

print( A.p1 , A.p2)
#print( B.b1, B.p1, B.p2)
print( C.c1, C.p1, C.p2)

"""- Q2. numpy & pandas """

import numpy as np
data = np.array([[1,2,3],[4,5,6]])

# reverse in numpy
z = np.arange(10)
reverse = z[::-1]
# 행바꾸기 
row = data[[1,0]]
# 열도 바꾸기 
col = data[[1,0]][:,[2,1,0]] 

print(reverse)
print(row)
print(col)

import pandas as pd

dic = {'도시': ['서울', '부산', '대전', '대구', '광주'],
        'year': [2017, 2017, 2018, 2018, 2018],
        'temp': [18, 20, 19, 21, 20]}

data = pd.DataFrame(dic) 
data

# 특정 행 얻는 법 - Index(pandas에서의 row) or 우리가 아는 index로  
data.index = ['a','b','c','d','e'] 

# method 1 행 이름으로 ! 
print( data.loc['a'] )  
print()
print( data.loc['a' : 'c']) # 'c' 포함
print()

# method 2 행 번호로 !
print( data.iloc[0] )
print()
print( data.iloc[0:2] ) # 2 포함 XX !

# 특정 열 얻는 법 
print()
print( data['year']) # numpy index처럼 바로 고냥 갖다박으면 된다.

# 열 추가 
data['New'] = ['1','2','3','4','5']

# 열 삭제
data = data.drop(['New'])

# in place
data.set_index(['도시'], inplace=True) 
data # inplace=False면 안바뀌고 고대로 ~

import numpy as np 

df = pd.DataFrame(np.arange(12).reshape(4, 3), 
                  columns=['A', 'B', 'C'], index=['a', 'b', 'c', 'd'])

# numpy와 달리 pandas의 default는 axis = 0이다. 전체 XX !!
print(df.max())
print()
print(df.max(0))
print()
print(df.max(1))

# dataframe 정렬 
df = pd.DataFrame(np.arange(8).reshape((2, 4)), index=['three', 'one'],
                  columns=['d', 'a', 'b', 'c'])

# 행 header 정렬
df.sort_index() 

# 열 header 정렬
df.sort_index(axis = 1)  # df.sort_columne 같은 건 없다 .... 개 페이크

# 열 value 정렬
frame = pd.DataFrame({'b': [4,7,3,2], 'a': [4,9,2,5], 'c': [5,3,7,9]}) 

frame.sort_values(by = 'a') # a 열의 값을 기준으로 정렬 나머지를 따라오기
frame.sort_values(by = 'b') # b 열의 값을 기준으로 정렬 나머지를 따라오기

obj = pd.Series([100, 33, 99, 33])

# 순위 매기기 --- 동점자는 평균값 줌
obj.rank(ascending=False) 

# 동일한 값이 존재 할 경우 먼저 나타나는 것에게 높은 순위를 줄 수 있다
obj.rank(method='first', ascending=False)

df = pd.DataFrame({'b': [4,7,3,2], 'a': [4,9,2,5], 'c': [5,3,7,9]})

# ---------- df에도 rank 수행할 수 있는데, defualt인 axis = 0이 열기준이다 다른 것과 달리 조금 이상하지만 참고하자.
df.rank() # 열 기준으로 순위 매기기
df.rank(axis=1) # 행 기준으로 순위 매기기

# Nan Value 처리
from numpy import nan as NA
df = pd.DataFrame([[NA, 6.5, 3.], [NA, NA, NA],
                  [NA, NA, NA], [NA, 6.5, 3.]])

# 한 항목이라도 NA가 있으면 해당 행을 삭제한다
cleaned = df.dropna()
# 한 항목이라도 NA가 있으면 해당 열을 삭제한다
clean2 = df.dropna(axis=1)

# 행의 모든 항목이 NA일때 해당 행을 삭제한다
df.dropna(how='all')
# 열의 모든 항목이 NA일때 해당 열을 삭제한다
clean2 = df.dropna(axis=1, how='all')

# 해당 행에 2개 이상 Nan이 있으면 삭제 
df.dropna(thresh=2) # Keep only the rows with at least 2 non-NA values.

# 컬럼별로 다른 값을 채울 수 있다. 사전을 사용한다
df.fillna({1: 0.5, 2: -1})

"""- Q3. HTML"""

from bs4 import BeautifulSoup

html_text = """
<html>
<body>
  <h1> reading web page with python </h1>
     <p> page analysis </p>
     <p> page alignment </p>
     <td>some text</td><td></td><td><p>more text</p></td><td>even <p>more text</p></td>
</body>
</html>
"""
# <td> : table 

soup = BeautifulSoup(html_text, 'html.parser')

# 기본 method 
print(soup.h1) # <h1> 에 대한 string 가져오기
print(soup.h1.text) # <h1> 에 대한 string을 각주 빼고 get
print(soup.h1.text.strip()) # 공백 문자 제거 

print(soup.p)
print(soup.p.next_sibling.next_sibling.text.strip())

# Tag로 가져오기 --- 더 많이 사용 
html_text2 = """
<html>
<body>
  <h1 id="title"> reading web page with python </h1>
     <p id="body"> page analysis </p>
     <p> page alignment </p>
     <td>some text</td><td></td><td><p>more text</p></td><td>even <p>more text</p></td>
     <ul>
         <li><a href = "http://www.naver.com"> naver</a></li>
         <li><a href = "http://www.daum.net"> daum</a></li>
     </ul>
  <div id="xxx">
    <h1> Wiki-books store </h1>
    <p> page alignment22 </p>
    <ul class="item">
      <li> introduction to game design </li>
      <li> introduction to python </li>
      <li> introduction to web design </li>
    </ul>
  </div>
</body>
</html>
"""
soup = BeautifulSoup(html_text2, 'html.parser')

soup.find(id = 'title')
soup.find(id = 'title').text.strip()

soup.find_all('p') # <p> 붙어 있는 놈 다 가져와라 --- list 로 가져옴

# <li><a href = "http://www.naver.com"> naver</a></li>
soup.find('a').attrs # 해당 태그에 붙어있는 속성 ---  {'href': 'http://www.naver.com'}

for aa in soup.find_all('a'): # 모든 <a>를 가져오고, 하나씩 iteration 
    href = aa.attrs['href'] # dict 형태 # <a>의 속성
    text = aa.string # <a>의 내용 
    print(text, "-->", href)

'''
naver --> http://www.naver.com
daum --> http://www.daum.net
'''

# find with Regular Expression 
import re
soup.find_all(re.compile("^p"))  # <p> 다 가져오라
soup.find_all(re.compile("div")) # div 다 가져와라
soup.find_all(href = re.compile("^http://")) # href가 http 인 놈 다 가져와라

# CSS method --- select 함수 사용

# find_all == select
soup.find('h1') # 얘는 하나만 찾는다. 다 찾으려면 find_all 사용 
soup.select('h1') # 이것도 tag로 찾는다. but 다 찾는다.

#  하위에 있는 내용들 싹다 가져온다
soup.select('#xxx')  # by id 
soup.select('.item') # by class name
soup.select('div .item')  # (tag=div, class=item)  --- class는 . 붙여서 구별 
soup.select("div li")   # hierarchy div 안에 있는 li 모두 return ( div >> li )

soup.select_one("#xxx > ul > li")  # hierarchy! id xxx 안에 태크 ul 안에 태그 li를 하나만 return

text = '''<p class="body strikeout"> hello </p>

          <p class="body strikeout"> hello2 </p>
          <p class="body"> hello3 </p>
'''

css_soup = BeautifulSoup(text, 'html.parser')
css_soup.find_all("p", class_="strikeout")  # p 타입의 class strikeout class 모두 다 가져오기 
css_soup.find_all("p", class_="body")
# class에 여러 가지 값이 있을 수 있다. class_ = body로 불러도 되고 class_ = strikeout 으로 불러도 된다.

# Example

import requests # internet에서 file을 가져오는 라이브러리 
from bs4 import BeautifulSoup

url = 'https://kr.indeed.com/jobs?q=data+science&l=%EC%84%9C%EC%9A%B8%ED%8A%B9%EB%B3%84%EC%8B%9C'
link = requests.get(url)
soup = BeautifulSoup(link.text, 'html.parser')

job_elems = soup.select('.resultContent') # 해당하는 이름의 class 싹다 list로 가져오기 --- select 자너 

for i in job_elems:

    # find는 하나만 가져온다 
    title = i.find('h2') # h2 태그 찾기 
    company = i.find('span', class_='companyName') # span 타입의 class companyName 태그 찾기
    location = i.find('div', class_='companyLocation') # div 타입의 class companyLocation 태그 찾기
    
    if None in (title, company, location):
        continue
        
    print(title.text.strip())
    print()
    print(company.text.strip())
    print(location.text.strip())

"""- Q4. BOW"""

import numpy as np
import pandas as pd

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

# CounterVectorizor : Corpus에서 나오는 단어를 모두 모아서 Colume으로 만듬 ( 행 : document , 열 : 단어 )
# 각 Document에서 Columne에 해당하는 단어가 몇 번 나오는 지 Count

corpus = [
    'This is the first document',
    'This document is the second document.',
    'And this is the third one.',
    'Is this the first document?'
]

vect = CountVectorizer()
# vect = CountVectorizer(min_df=10)  최소한 10번은 등장해야 단어로 보겠다.
X = vect.fit_transform(corpus) # 단어가 몇 개 나오는지 Count 
print(X.toarray())
print(vect.get_feature_names())
'''
[[0 1 1 1 0 0 1 0 1]
 [0 2 0 1 0 1 1 0 1]
 [1 0 0 1 1 0 1 1 1]
 [0 1 1 1 0 0 1 0 1]]

['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']
'''

# TFIDF : log ( document 개수 / 해당 단어를 가진 document 개수 )   ( 독 / 단 )
vect = TfidfVectorizer() 
X = vect.fit_transform(corpus)
print(X.toarray().round(1))
print(vect.get_feature_names()) # 알파벳 순서로 정렬되므로, 단어들 간의 순서와 서로 간의 의미가 무시된다.
'''
[[0.  0.5 0.6 0.4 0.  0.  0.4 0.  0.4]
 [0.  0.7 0.  0.3 0.  0.5 0.3 0.  0.3]
 [0.5 0.  0.  0.3 0.5 0.  0.3 0.5 0.3]
 [0.  0.5 0.6 0.4 0.  0.  0.4 0.  0.4]]
['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']
'''

# stop words 흔한 단어, 필요없는 단어  ngram_range : 1개 2개 1개 2개 ... 
vect = TfidfVectorizer(ngram_range=[1,2], stop_words='english') 
X = vect.fit_transform(corpus)
print(X.toarray().round(1))
print(vect.get_feature_names())

idx = X.toarray().sum(0).argsort()
idx

iidx = X.toarray().sum(0).argsort()[-5:] # 가장 많이 나온거 5개의 index return 
iidx
#array([ 7, 12,  6, 18,  2])

"""- Q5. Missing Value"""

import numpy as np
import pandas as pd

from numpy import nan as NA
df = pd.DataFrame([[NA, 6.5, 3.], [NA, NA, NA],
                  [NA, NA, NA], [NA, 6.5, 3.]])

df.isna().sum() # 각 열에 대해서 NA의 개수 return   ---- rank와 마찬가지로 이놈도 default가 열 기준

df

df.isna().sum().sort_values(ascending=False)

"""- One hot Encoding
- Ordinal Encoding
- Label Encoding 
"""

# One-hot Encoding : Nominal data 
# Label / Ordianl Encoding : Ordinal data 
# Label Encoding : to Target Value
# One-hot / Ordinal Encoding : to feature Values

X, y = df.values[:, :-1], df.values[:,-1]
oe = OrdinalEncoder()
X_enc = oe.fit_transform(X)
le = LabelEncoder()
y_enc = le.fit_transform(y)
oe.categories_, le.classes_

"""- Standard Scaling

      z = (x - mean) / std

- Min - Max Scaling

      z = x - min / max - min 

- Robust Scaling

      z = x - median / IQR

"""

df = pd.DataFrame({
    'x1': np.random.normal(0, 2, 10000),
    'x2': np.random.normal(5, 3, 10000),
    'x3': np.random.normal(-5, 5, 10000)
})
df.head()
df.plot.kde()  # kernel density estimate

from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
ss = StandardScaler()
data_tf = ss.fit_transform(df)     # returns an array
df = pd.DataFrame(data_tf, columns=['x1','x2','x3'])
df.plot.kde()

import matplotlib.pyplot as plt
mm = MinMaxScaler()
data_tf = mm.fit_transform(df)
df1 = pd.DataFrame(data_tf,columns=['x1','x2','x3'])

sc = StandardScaler()
data_tf = sc.fit_transform(df)
df2 = pd.DataFrame(data_tf,columns=['x1','x2','x3'])

rb = RobustScaler()
data_tf = rb.fit_transform(df)
df3 = pd.DataFrame(data_tf,columns=['x1','x2','x3'])

fig, axes = plt.subplots(1, 3, figsize=(16,6))
df1.plot.kde(ax=axes[0])
df2.plot.kde(ax=axes[1])
df3.plot.kde(ax=axes[2])

"""- Q6. GD"""

w1 = np.random.randn()
w2 = np.random.randn()
b  = np.random.randn() 

def sigmoid_activation(z):
    return 1.0 / (1 + np.exp(-z))

lossHistory = []
epochs = 300
alpha = 0.01

for epoch in np.arange(epochs):
    preds = sigmoid_activation(w1*x1 + w2*x2 + b)       # prediction

    loss = -( ( y*np.log(preds) + (1-y)*np.log(1-preds) ) ).mean()  # loss = cross entropy
    lossHistory.append(loss)
    
    dloss_dz = preds - y
    w1_deriv = dloss_dz * x1        # d(loss)/dw1 = d(loss)/dz * dz/dw1
    w2_deriv = dloss_dz * x2
    b_deriv = dloss_dz * 1
    
    w1 = w1 - (alpha * w1_deriv).mean()
    w2 = w2 - (alpha * w2_deriv).mean()
    b  = b  - (alpha * b_deriv).mean()

print(w1, w2, b)
accuracy = ((sigmoid_activation(w1*x1 + w2*x2 + b) > 0.5) == y).sum()/N
print(accuracy)